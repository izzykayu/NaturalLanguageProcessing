{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "### BMSC-GA 4493, BMIN-GA 3007 \n",
    "### Homework 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex \n",
    "\n",
    "See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "**Submission instruction**: Upload and Submit your final jupyter notebook file in <a href='http://newclasses.nyu.edu'>newclasses.nyu.edu</a>\n",
    "\n",
    "**Submission deadline:** Tuesday Feb 13th 2018 (3:00 PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Take Derivatives!  (Total 25 points)\n",
    "### Take derivatives of function f(x) with respect to x in questions 1.1 to 1.7. For 1.8, take partial derivatives of f(X, A) with respect to each $a_i$ and $x_i$. (3 points for 1.1 to 1.7 and 4 points for 1.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) $f(x) = x^2 + 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) $f(x) = sin(x)+tanh(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) $f(x) = log_e(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) $f(x) = e^{2x + 5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5) $f(x) = \\sum_{i=1}^{4}log_e(a_i x^2 + b_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($a_1$, $a_2$, $a_3$ and $a_4$ are constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6) $f(x) = \\sqrt{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7) $f(x) = \\sqrt{\\sum_{i=1}^{4}(a_i x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($a_1$, $a_2$, $a_3$ and $a_4$ are constants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8) Now consider $X$ is a d-dimensional variable. i.e. $X=(x_1, x_2, ... , x_d)$. Consider $A = (a_i, a_1, ..., a_d)$ to also be a variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute partial derivative of $f(X,A)$ with respect to each $x_i$, and each $a_i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(X, A) = \\sum_{i=1}^{d}{log_e(a_ix_i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Solving Linear Regression via Mean Squared Error (MSE) Optimization Problem (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that you have measured two variables X and Y, for a simple task, and you belive that they might be linearly related to each other.\t\n",
    "The measurements are as follows:\n",
    "\n",
    "###### (Training data D = {($X_1$, $Y_1$), ($X_2$, $Y_2$), ($X_3$, $Y_3$)})\n",
    "\n",
    "Data point 1: $X_1$ = 2, $Y_1$ = 5\n",
    "\n",
    "Data point 2: $X_2$ = 4, $Y_2$ = 9\n",
    "\n",
    "Data point 3: $X_3$ = 5, $Y_3$ = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the relationship between X and Y is linear, we can write this relationship as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y = f_{W,B}(X) = WX + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $W$ and $B$ are the parameters of the model.\t\n",
    "We are interested in finding best values for W and B.\t\n",
    "We define 'best' in terms of a loss function between $f_{W,b}(X_i)$ and $Y_i$ for each ($X_i$ and $Y_i$) in the training data. \t\n",
    "Since $Y_i$s are real numbers, let's consider Mean Squared Error loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that Mean Squared Error for this function, over training data, and W and B is:\n",
    "\n",
    "$MSELoss(D={(X_1, Y_1), (X_2, Y_2), (X_3, Y_3)}), W, B) = \\frac{1}{3}\\sum_{i=1}^{3} (f_{W,B}(X_i) - Y_i)^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) (6 points) \n",
    "Compute the partial derivative of $MESLoss(D, W, B)$, With respect to W and B.\t\n",
    "Remember that $X_1$, $X_2$, $X_3$, $Y_1$, $Y_2$, and $Y_3$ are constants, and already given to us as training data above.\n",
    "\n",
    "$\\frac{d}{d W} MSELoss(D, W, B) = ?$\n",
    "\n",
    "$\\frac{d}{d B} MSELoss(D, W, B) = ?$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) (3 points) \n",
    "Use matplotlib library and plot $\\frac{d}{d W} MSELoss(D, W, B)$ for W=range(10), when B equals to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) (3 points) \n",
    "What values of W and B, make both partial derivatives zero? \t\n",
    "i.e. Solve and find the unique answer to $\\frac{d}{d W} MSELoss(D, W, B) = 0$ , and $\\frac{d}{d B} MSELoss(D, W, B) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) (8 points) \n",
    "If you start from an initial point $W_0$ = 0.1 and $B_0$ = 0.1, and iteratively update your W and B via gradient descent as follows:\n",
    "    \n",
    "\n",
    "$ W_{t+1} = W_t - 0.01 * \\frac{d}{d W} MSELoss(D, W, B) |_{W_t,B_t} $\t\n",
    "$ B_{t+1} = B_t - 0.01 * \\frac{d}{d B} MSELoss(D, W, B) |_{W_t,B_t} $\t\n",
    "(Note: This is gradient descent with a 0.01 learning rate.)\n",
    "\n",
    "What are the values of W and B over iterations 0 to 5000? (Don't compute by hand! Write a code!)\t\n",
    "Write a python script that computes these values for 5000 iterations, i.e. lists of $\\{W_0, W_1,.., W_{5000}\\}$, and $\\{B_0, B_1,.., B_{5000}\\}$.\t\n",
    "Plot the lists of W and B over 5000 iterations here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5) (10 points) \n",
    "Now that you learned the math and made the code yourself, we will use pytorch and automatic differentiation, to find optimal W and B!\t\n",
    "Again, consider data to be D = {($X_1$, $Y_1$), ($X_2$, $Y_2$), ($X_3$, $Y_3$)}) = {(2,5), (4,9), (5, 11)}.\n",
    "\n",
    "Some of your steps are here. Fill in the rest and show a plot of the loss function, W and B over these 500 epochs. (3 plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "D = [(2,5), (4,9), (5, 11)]\n",
    "X = [d[0] for d in D]\n",
    "Y = [d[1] for d in D]\n",
    "print(X, Y)\n",
    "\n",
    "model = torch.nn.Linear(1, 1, bias=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(500):\n",
    "    for i in range(3):\n",
    "        xinput = Variable(torch.from_numpy(np.array([X[i]]))).type(torch.FloatTensor)\n",
    "        ytarget = Variable(torch.from_numpy(np.array([Y[i]]))).type(torch.FloatTensor)\n",
    "        # don't forget to zero_grad your model. \n",
    "        # forward into your model and loss\n",
    "        # do a backward step to compute gradients\n",
    "        # make one step with the optimizer\n",
    "        # keep track of the loss, W and b in some lists.\n",
    "\n",
    "#Plot loss, W and b lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Solving Classification - Logistic Regression - via Negative Log Likelihood Optimization (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Imagine that your still have measured two variables X and Y, for a simple task, but your output $Y$ is only either 0 or 1.\t\n",
    "This is called classification. Our observations are as follows:\n",
    "\n",
    "##### Training data D = {($X_1$, $Y_1$), ($X_2$, $Y_2$), ($X_3$, $Y_3$)}\n",
    "\n",
    "Data point 1: $X_1$ = 2, $Y_1$ = 0\t\n",
    "Data point 2: $X_2$ = 4, $Y_2$ = 0\t\n",
    "Data point 3: $X_3$ = 5, $Y_3$ = 1\t\n",
    "\n",
    "How can we think of a function, f(X), which gives us binary predictions?\t\n",
    "Often, solution is to try to model probability of the label of X being equal to 1 (or 0).\t\n",
    "In other words, we can try to model:\n",
    "\n",
    "#### $P(Y=1|X) = f(X)$\n",
    "\n",
    "Probabilities are numbers between 0 and 1, so often, people use a function Sigmoid (<a href=\"https://en.wikipedia.org/wiki/Sigmoid_function\">Read More</a>) on the output of a linear function, to map an input, X, to the probability of its label, Y, being equal to 1. \n",
    "\n",
    "$P_{W,B}(Y=1|X) = f_{W,B}(X) = \\frac{1}{1+e^{-(WX+B)}}$\n",
    "\n",
    "This is the basic formulation of the simplest classification model: Logistic Regression!\t\n",
    "You can note, that this function $\\frac{1}{1+e^{-(WX+B)}}$ is also parametrized with a W and B only.\t\n",
    "Similar to Question 2, we can also find the 'best' W and B, by optimizing some loss function over the training data. \n",
    "\n",
    "In Classification tasks, the common loss functin to use is negative log likelihood,\t\n",
    "which is simply the negative of sum of log of probabilities of observed samples taking their correct labels. i.e.\n",
    "\n",
    "$NLL\\_Loss(D,W,B) =  -Log(P_{W,B}(Y=0|X_1)) -Log(P_{W,B}(Y=0|X_2)) - Log( P_{W,B}(Y=1|X_3))$\n",
    "\n",
    "By expanding $P_{W,B}(Y=1|X) = \\frac{1}{1+e^{-(WX+B)}}$, and $P_{W,B}(Y=0|X) = 1- \\frac{1}{1+e^{-(WX+B)}}$, we can use chain rule and backpropagation to compute derivative of $NLL\\_Loss(D,W,B)$ with respect to W and B, and find the 'best' W and B for each given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) (6 points)\n",
    "What are $\\frac{d}{d W} NLL\\_Loss(D,W,B)$, and $\\frac{d}{d B} NLL\\_Loss(D,W,B)$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2) (8 points)\n",
    "If you start from an initial point $W_0$ = 0 and $B_0$ = 0, and iteratively update your W and B via gradient descent as follows:\n",
    "    \n",
    "$ W_{t+1} = W_t - 0.01 *  \\frac{d}{d W} NLL\\_Loss(D,W,B) |_{W_t,B_t} $\t\n",
    "$ B_{t+1} = B_t - 0.01 * \\frac{d}{d W} NLL\\_Loss(D,W,B) |_{W_t,B_t} $\n",
    "\n",
    "what are the values of W and B over iterations 0 to 500? (Don't compute by hand!)\t\n",
    "Write a script that computes these values for 500 iterations, and plot these lists of $\\{W_0, W_1,.., W_{500}\\}$, and $\\{B_0, B_1,.., B_{500}\\}$ via matplotlib here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3) (10 points) \n",
    "Use pytorch to implement Logistic Regression! We write the first parts of it. You fill in the rest. Plot W and B and value of the loss function over these 500 iterations. (3 plots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "\n",
    "D = [(2,0), (4,0), (5, 1)]\n",
    "X = [d[0] for d in D]\n",
    "Y = [d[1] for d in D]\n",
    "print(X, Y)\n",
    "\n",
    "model = #?\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss = #?\n",
    "\n",
    "for epoch in range(500):\n",
    "    for i in range(3):\n",
    "        xinput = Variable(torch.from_numpy(np.array([X[i]]))).type(torch.FloatTensor)\n",
    "        ytarget = Variable(torch.from_numpy(np.array([Y[i]]))).type(torch.FloatTensor)\n",
    "        # don't forget to zero_grad your model. \n",
    "        # forward input into your model and loss\n",
    "        # do a backward step to compute gradients\n",
    "        # make one step with the optimizer\n",
    "        # keep track of the loss, W and b in some lists.\n",
    "\n",
    "#Plot loss, W and b lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Learning Curves, Overfitting, and Machine Learning! \n",
    "# (34 points +10 Bonus points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to optimize, let's get some real machine learning done!\t\n",
    "\n",
    "Instead of the small dataset we had in questions 2 and 3, now let's use the the CBIS-DDSM (Curated Breast Imaging Subset of DDSM) dataset from <a href=\"https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM#385f2cd4e86f4142b1d32bdb5803bd96\"> here</a> (Click on the 'Detailed Description' tab at the bottom of the page.)\n",
    "\n",
    "\n",
    "In this homework, we will *only* focus on the following items in the dataset:\t\n",
    "Mass-Training-Description (csv)\t\n",
    "Mass-Test-Description (csv)\t\n",
    "(Don't download the images on your laptop! That file is too big and we deal with it on the cluster later!)\n",
    "\n",
    "This dataset contains several features related to Mammography and detection of breast cancer. \n",
    "\n",
    "The Mass-Training-Description and Mass-Test-Description include these columns:\n",
    "\n",
    "patient_id\t\n",
    "breast_density\t\n",
    "left or right breast\t\n",
    "image view\t\t\n",
    "abnormality id\t\t\n",
    "abnormality type\t\n",
    "mass shape\t\n",
    "mass margins\t\n",
    "assessment\t\n",
    "pathology\n",
    "\n",
    "There is more data in this dataset, including images, but for this homework we will not focus on them.\n",
    "\n",
    "We are interested in this question:\t\n",
    "Using variables:\t\n",
    "\n",
    "breast_density\t\n",
    "left or right breast\t\n",
    "image view\t\t\n",
    "abnormality id\t\t\n",
    "abnormality type\t\n",
    "mass shape\t\n",
    "mass margins\t\n",
    "\n",
    "How well can we predict the **pathology type**?\n",
    "\n",
    "We can answer that by training a model on the Mass-Training-Description, and evaluating it on Mass-Test-Description. \n",
    "See questions 4.1 and 4.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1) (10 points)\n",
    "Write a script to convert the data from variables [breast_density, left or right breast, image view, abnormality id,\n",
    "abnormality type, mass shape, mass margins] into input and [pathology type] into output.\n",
    "\n",
    "The output of your script should be a matrix X and a vector Y,\t\n",
    "where each row of X are one set of variables for a patient\t\n",
    "and each row of Y is the pathology type class, for that patient.\t\n",
    "\n",
    "Use *matplotlib.imshow(X, aspect='auto')* to visualize the X.\t\n",
    "(And if there are multiple equivalent rows per patient, keep only one of them - any, up to you)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 (4 points)\n",
    "Repeat Question 4.1 for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 (10 points)\n",
    "Write your training script for a multi-layered-perceptron classifier with CrossEntropy loss.\n",
    "Plot the ***average loss on all the train samples*** per epoch. (Stop the training after 100 epochs. You are welcome to compute for more than 100, however. Up to you.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 (10 points)\n",
    "Add a test-set evaluation of the loss in your answer to 4.2, and plot the ***average loss on all the test samples*** per epoch. (Stop the training after 100 epochs. You are welcome to compute for more than 100, up to you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 (5 points)\n",
    "Change some of the hyper-parameters of your training - number of hidden nodes or layers - and plot the train and test loss per epoch.\n",
    "\n",
    "Also repeat your experiments with and without **normalization** of columns of test and train set.\n",
    "\n",
    "Note: You should only normalize the non-binary columns, usually.\t\n",
    "You can use something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_nonbinary_columns(x):\n",
    "    for ix in range(x.shape[1]):\n",
    "        if x[:,ix].min() != 0 or x[:,ix].max() != 1:\n",
    "            if  x[:,ix].std() != 0:\n",
    "                print( 'non-binary colunm!', ix)        \n",
    "                x[:,ix] = (x[:,ix] - x[:,ix].mean()) / x[:,ix].std() \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe what is the best final model that you have found. What were the configurations?\n",
    "What's the final best CrossEntropy loss on validation (test) set that you ever found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 (5 points)\n",
    "\n",
    "Add AUC computation to your evaluation at each epoch for test and train set. What is the best Area Under ROC curve for prediction of MALIGNANT class on the validation(test) set you ever found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3) (Bonus up to Max 10 points)\n",
    "Be creative and think about other interesting machine learning tasks that could be done with this dataset.\t\n",
    "Any interesting idea gives a bonus point of +2 up to the 10 points max.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
